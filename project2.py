#!/usr/bin/env python
# coding: utf-8

# # Описание проекта

# Я работаю в стартапе, который продаёт продукты питания. Нужно разобраться, как ведут себя пользователи нашего мобильного приложения. 
# Изучитить воронку продаж. Узнать, как пользователи доходят до покупки. Сколько пользователей доходит до покупки, а сколько — «застревает» на предыдущих шагах? На каких именно?
# После этого исследовать результаты A/A/B-эксперимента и выяснить, какой шрифт для сайта лучше? Стоит ли менять шрифты?
# 

# ## Загрузка и обзор данных

# In[1]:


import pandas as pd
import datetime as dt
import numpy as np
import matplotlib.pyplot as plt
from pandas.plotting import register_matplotlib_converters
import warnings
from scipy import stats as st
import math as mth
import cmath
from plotly import graph_objects as go
import seaborn as sns


# In[2]:


data = pd.read_csv('/datasets/logs_exp.csv', sep='\t')


# In[3]:


data.head(10)


# In[4]:


data.info()


# In[5]:


pd.DataFrame(round(data.isna().mean()*100,)).style.background_gradient('coolwarm')


# In[6]:


data.duplicated().sum() / data['ExpId'].count() *100


# На обзоре данных можно заметить, что пропуски отсутствуют, но имеются дубликаты которые составляют(0,16%) от всех данных. Их можно удалить, так как они не повлияют на дальнейшее иследование. Название столбцов лучше привест к нижнему регистру и переименомать для понимания. Также следует привести некоторые столбцы к нужному формату.

# ## Предобработка данных

# In[7]:


data = data.drop_duplicates().reset_index(drop=True)


# In[8]:


data.columns = ['event_name','id','event_time','experiment']


# In[9]:


data['date_time'] = pd.to_datetime(data['event_time'], unit='s')
data['date'] = data['date_time'].astype('datetime64[D]')


# In[10]:


gr_246 = data[data['experiment']==246]['id']
gr_247 = data[data['experiment']==247]['id']
gr_248 = data[data['experiment']==248]['id']


# In[11]:


cnt_1 = 0
for i in gr_246:
    if i in gr_247:
        cnt_1 += 1
print(cnt_1)


# In[12]:


cnt_2 = 0
for i in gr_246:
    if i in gr_248:
        cnt_2 += 1
print(cnt_2)


# In[13]:


cnt_3 = 0
for i in gr_247:
    if i in gr_248:
        cnt_3 += 1
print(cnt_3)


# На этапе предобработки я изменил названия столбцов на более удобные, удалил дубликаты, и привел столбец со временем к нужному формату. Также, добавил столбец с датой без времени. Он необходим для дальнейшей работы.
# 
# Одни и те же пользователи не попали в разные группы.

# ## Изучение и проверка данных

# In[14]:


data['date'].count()


# In[15]:


data['id'].drop_duplicates().count()


# In[16]:


round(data.groupby('id')['date'].count().mean(),2)


# In[17]:


data.groupby('id')['date'].count().median()


# In[18]:


data_gr = data.groupby('id', as_index=False).agg({'event_name':'count'})
data_gr


# In[52]:


data_gr['event_name'].hist(range=(0,300));


# In[20]:


data['date'].max(), data['date'].min()


# In[22]:


data['date_time'].hist(bins=14*24, figsize=(14, 5));


# Из диаграммы видно, что данные за июль неполные, поэтому было принято решение удалить их. Однако, пользователи которые совершали заказы в июле, могли продолжить активность в августе, поэтому их сделуют тоже убрать из иследования, чтобы избежать перекоса данных.

# In[23]:


data_new = data[data['date'] >= '2019-08-01']


# In[24]:


data_new['date'].hist(figsize=(10,5));


# После того как избавились от неполных данных видно, что события расположились равномерно. 

# In[25]:


data['date'].count() - data_new['date'].count()


# In[26]:


round((data['date'].count() 
       - data_new['date'].count()) / data['date'].count() *100,2)


# In[27]:


data['id'].drop_duplicates().count() - data_new['id'].drop_duplicates().count()


# In[28]:


round((data['id'].drop_duplicates().count()
       - data_new['id'].drop_duplicates().count()) /data['id'].drop_duplicates().count() *100,2)


# In[29]:


data_new.groupby('experiment')['id'].count()


# Первоночально в таблице было 244126 событий. 7551 уникальных пользователей. В среднем на пользователя приходится 32 события, а медианное значение 20.
# Мы распологаем данными за период с 2019-07-25 по 2019-08-07. Однако данные за  июль неполные, поэтому их пришлось удалить вместе с пользователями, которые совершали заказы в июле, чтобы избежать перекоса данных.
# Отбросим их мы потеряли 60173 события и 1455 уникальных пользователя.
# Что составляет 25% событий и практически 20% пользователей. Довольно много, но это необходимо для достоверности анализируемых данных.
# В данных присутсвуют пользователи из всех 3х групп. 
# В группе 248 пользователей больше примерно на 7000 чем в остальных.

# ## Исследование воронки событий

# In[30]:


data_new.head()


# In[31]:


funnel = data_new.groupby('event_name').agg({'id' : 'count'}) .sort_values(by='id',ascending=False)
funnel = funnel.drop('Tutorial')
funnel['unique'] = data_new.groupby('event_name')['id'].nunique().sort_values(ascending=False)


# In[32]:


round(funnel.assign(shape = lambda x: x['unique'] / x['unique'].shift()*100),2)


# Tutorial имеет меньше всего событий, но по логике должен быть первым. Видимо многие пропускают обучение.                   

# Всего 3539 уникальных пользователя дошло до конца и завершило оплату.

# In[33]:


data_new['id'].nunique()


# всего уникальных пользователей - 7534, скорее всего 15 пользователя сталкнулись с ошибкой или закрыли сайт еще во время обучения.

# 47% уникальных пользователей успешно прошли все этапы. На экран продукта 61.91% пользователей. На экран корзины 81.30% пользователей. Оплата завершена у 94.78% пользователей.

# In[34]:


round(funnel['unique'].tail(1).sum()/funnel['unique'].head(1).sum()*100,2)


# In[35]:



fig = go.Figure(go.Funnel(
    y = funnel.index,
    x = funnel['unique']))
fig.update_layout(title='Воронка событий')
fig.show()


# Доля пользователей доходит от первого события до оплаты 47.4%. Менее половины пользователей проходят всю воронку.

# В логах присутствуют 5 событий в таком порядке: Tutorial(обучение), MainScreenAppear(главный экран), OffersScreenAppear(экран оформления заказа), CartScreenAppear(экран оплаты), PaymentScreenSuccessful(экран завершения оплаты), но "обучение" не встаивается в последовательную цепочку поэтому мы его отбрасываем. Видимо многие пропускают обучение.
# 
# Больше всего потерь наблюдается при переходе с главного экрана на экран товаров. Скорее всего есть проблемы с главной страницей. Ее необходимо доработать или изменить. 38% отпадают на этом этапе.
# 
# В целом менее половины пользователей проходят весь этап от попадания на сайт до покупки.

# ## Проработка результатов эксперемента

# In[36]:


data_new.groupby('experiment').agg(count=('id', 'nunique'))


# Количество уникальных пользователей по группам практически равное.

# In[37]:


def z_test(df_1,df_2,event,alpha):
    alpha=alpha

    users = np.array([df_1['id'].nunique(),df_2['id'].nunique()])


    leads = np.array([df_1[df_1['event_name']==event]['id'].nunique(),
                      df_2[df_2['event_name']==event]['id'].nunique()])
                      
                      
    # пропорция успехов в первой группе:
    p1 = users[0]/leads[0]

    # пропорция успехов во второй группе:
    p2 = users[1]/leads[1]

    # пропорция успехов в комбинированном датасете:
    p_combined = (users[0] + users[1]) / (leads[0] + leads[1])

    # разница пропорций в датасетах
    difference = p1 - p2 

    # считаем статистику в ст.отклонениях стандартного нормального распределения
    z_value = difference / cmath.sqrt(p_combined * (1 - p_combined) * (1/leads[0] + 1/leads[1]))

    # задаем стандартное нормальное распределение (среднее 0, ст.отклонение 1)
    distr = st.norm(0, 1)  

    p_value = (1 - distr.cdf(abs(z_value))) * 2

    print(event)
    print('p-значение: ', p_value)

    if p_value < alpha:
        print('Отвергаем нулевую гипотезу: между долями есть значимая разница')
    else:
        print(
            'Не получилось отвергнуть нулевую гипотезу, нет оснований считать доли разными'
        )


# In[38]:


data_new = data_new[data_new['event_name'] != 'Tutorial']


# In[39]:


types_of_events = data_new['event_name'].unique()
exp_246=data_new[data_new['experiment'] == 246]
exp_247=data_new[data_new['experiment'] == 247]
exp_248=data_new[data_new['experiment'] == 248]
exp_contr=pd.merge(exp_246, exp_247, how='outer') 
alpha=0.05


# Нам нужно провести 16 статистических теста(4 парных теста по 4м событиям) с уровнем значимости 5%.
# Метод Бонферрони = 0.05/16 = 0.003
# принимаем за уровень значимости значение 0.003

# In[44]:


alpha=0.003


# Нулевая гипотиза: различий в долях между выборками контрольной группы нет. Альтернативная гипотиза: различия в долях между выборками контрольной группы есть.

# In[45]:


for event in types_of_events:
    z_test(exp_246,exp_247,event,alpha)


# Нулевая гипотиза: различий в долях между выборками 246 и 248 нет. Альтернативная гипотиза: различия в долях между выборками 246 и 248 есть.

# In[46]:


for event in types_of_events:
    z_test(exp_246,exp_248,event,alpha)


# Нулевая гипотиза: различий в долях между выборками 247 и 248 нет. Альтернативная гипотиза: различия в долях между выборками 247 и 248 есть.

# In[47]:


for event in types_of_events:
    z_test(exp_247,exp_248,event,alpha)


# Нулевая гипотиза: различий в долях между выборками контрольной группы и альтернативной нет. Альтернативная гипотиза: различия в долях между выборками контрольной группы и альтернативной есть.

# In[48]:


for event in types_of_events:
    z_test(exp_contr,exp_248,event,alpha)


# ## Общий вывод

# 1) На этапе обзоре данных были выявленны: что пропуски отсутствуют, имеются дубликаты (0,16%) от всех данных. В целом данные коректны.
# 
# 
# 2) На этапе предобработки были изменены: названия столбцов на более читаемые, удалены дубликаты, приведен столбец со временем к нужному формату, добавлен столбец с датой без времени.
# 
# 
# 3) На этапе изучения и проверки данных установленно: первоночально в таблице было 243713 событий, 7551 уникальных пользователей, в среднем на пользователя приходится 32 события, медианное значение 20, данные за период с 2019-07-25 по 2019-08-07, данные за июль неполные. Неполные данные были удалены, чтобы избежать перекоса данных. Итоговые даты теста с 2019-08-01 по 2019-08-07.
# По итогу потери составили: 2826 события, 17 уникальных пользователя (1.16% событий, 0.2% пользователей). В данных присутсвуют пользователи из всех 3х групп. Группы распределены примерны одинаково, но в 248 чуть больше пользователей.(примерно на 4 тысячи)
# 
# 
# 4) На этапе исследования воронки событий установлено: доля пользователей доходит от первого события до оплаты - 47.4% (менее половины пользователей проходят всю воронку), в логах присутствуют 5 событий в таком порядке: Tutorial(обучение), MainScreenAppear(главный экран), OffersScreenAppear(экран оформления заказа), CartScreenAppear(экран оплаты), PaymentScreenSuccessful(экран завершения оплаты), но "обучение" не встаивается в последовательную цепочку поэтому мы его отбрасываем. Видимо многие пропускают обучение.
# Больше всего потерь наблюдается при переходе с главного экрана на экран товаров. Скорее всего есть проблемы с главной страницей. Ее необходимо доработать или изменить. 38% отпадают на этом этапе.
# 
# 
# 5) На этапе проработки результатов теста выявленно, что количество уникальных пользователей по группам практически равное.
# Также, было проведено 16 статистических теста с уровнем значимости 10%. Тестирования показали, одинаковые результаты, где нуливые гипотезы не смогли быть отвергнуты, соответственно это доказывает, что тест можно считать достоверным. Посколько при уровне значимости 10% не удалось отвергнуть гипотезы, было принято установить его как подходящий для иследования.

# In[ ]:




